# Topic Trend Aggregator

## üß† What It Is

The `Topic Trend Aggregator` is the first major analysis stage in the AI Content Intelligence Ecosystem. It is a **stateful actor**, meaning it builds a long-term memory of news articles over time to identify dominant, persistent trends rather than just analyzing daily headlines. It ingests raw articles from various news pipelines and uses AI to discover, cluster, and score these emerging narratives.

## ‚ú® Key Features

* **Stateful Persistence**: The actor uses the Apify Key-Value Store to save its state between runs. It remembers articles it has already processed, allowing it to track the evolution of topics over time and avoid redundant analysis.
* **AI-Powered Analysis**: It leverages a powerful Large Language Model (`gpt-4o`) to analyze batches of article headlines and identify the single most dominant trend within a given category.
* **Efficient Data Handling**: The actor includes logic to automatically deduplicate new articles against its historical data based on URL. It also prunes its memory, keeping only the most recent articles per category (e.g., the last 500) to ensure its state remains manageable and relevant.

## ‚öôÔ∏è How It Works

The actor follows a cyclical process designed for scheduled, recurring runs:

1.  **Load State**: At the start of a run, the actor loads its previous state (`TREND_STATE`) from the Key-Value Store, which contains all the articles it has collected from past runs, organized by category.
2.  **Fetch New Articles**: It fetches the latest articles from the datasets of the upstream news pipeline actors specified in the input.
3.  **Deduplicate & Merge**: It filters out any articles it has already seen and merges the new, unique articles into its historical data.
4.  **LLM Analysis**: For each news category, it sends the most recent headlines to an LLM, which returns a structured JSON object identifying the dominant topic, a trend score, and key entities.
5.  **Push Results**: The identified trends are pushed to the actor's default dataset, creating a clean, prioritized list.
6.  **Prune & Save State**: Finally, the actor prunes older articles from its memory and saves the updated `TREND_STATE` back to the Key-Value Store, preparing it for the next run.

## üì• Inputs

The actor is configured through the following input fields:

* **`news_sources`**: An array where you specify which upstream news actors to pull data from. Each item in the array is an object containing the `actorId` and a `categoryName` for the source.
* **`max_articles`**: An integer that controls the maximum number of new articles to fetch from each source actor per run.
* **`OPENAI_API_KEY` (Secret)**: Your OpenAI API key is required for the AI-powered clustering and topic naming steps. It must be provided to enable the analysis.

## üì§ Outputs

The actor produces a new dataset where each row represents a distinct, long-term trend identified by the AI. The structure of each item is as follows:

* **`cluster_id`** (String): A unique identifier for the trend (e.g., `LONG_TERM_TREND_ARTIFICIAL_INTELLIGENCE_SECTOR`).
* **`cluster_topic`** (String): The human-readable name for the trend generated by the AI (e.g., "AI's Impact on Global Energy Consumption").
* **`trend_score`** (Number): A numerical score (typically 1-100) assigned by the AI reflecting the perceived importance and momentum of the topic.
* **`articles_count`** (Integer): The total number of unique articles that have been collected for this trend over time.
* **`articles`** (Array): A list of the individual article objects belonging to this trend cluster, each containing its title, URL, publication date, and other metadata.

This dataset serves as the direct input for the `AI Opportunity Scout`.